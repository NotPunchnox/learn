{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "lora_repo = \"punchnox/Aash\"  # LoRA model repository\n",
    "lora_path = \"AashModel\"  # Local path for LoRA model\n",
    "base_model_repo = \"Qwen/Qwen1.5-4B\"  # Base model repository\n",
    "base_model_path = \"Qwen3-4B\"  # Local path for base model\n",
    "llama_cpp_path = \"llama.cpp\"  # Local path for llama.cpp\n",
    "output_dir = \"AashModelMerged\"  # Output directory for merged model\n",
    "gguf_output_path = \"Aash-v2-merged.gguf\"  # Output GGUF file\n",
    "offload_dir = \"offload\"  # Directory for disk offloading\n",
    "\n",
    "# Step 1: Check and download LoRA model\n",
    "print(\"Checking for LoRA model...\")\n",
    "if not os.path.exists(lora_path) or not os.path.exists(os.path.join(lora_path, \"adapter_model.safetensors\")):\n",
    "    print(f\"LoRA model {lora_repo} not found. Downloading to {lora_path}...\")\n",
    "    snapshot_download(repo_id=lora_repo, local_dir=lora_path, local_dir_use_symlinks=False)\n",
    "else:\n",
    "    print(f\"LoRA model found at {lora_path}\")\n",
    "\n",
    "# Step 2: Check and download base model\n",
    "print(\"Checking for base model...\")\n",
    "if not os.path.exists(base_model_path) or not os.path.exists(os.path.join(base_model_path, \"config.json\")):\n",
    "    print(f\"Base model {base_model_repo} not found. Downloading to {base_model_path}...\")\n",
    "    snapshot_download(repo_id=base_model_repo, local_dir=base_model_path, local_dir_use_symlinks=False)\n",
    "else:\n",
    "    print(f\"Base model found at {base_model_path}\")\n",
    "\n",
    "# Step 3: Check and download llama.cpp\n",
    "print(\"Checking for llama.cpp...\")\n",
    "if not os.path.exists(llama_cpp_path) or not os.path.exists(os.path.join(llama_cpp_path, \"convert_hf_to_gguf.py\")):\n",
    "    print(f\"llama.cpp not found. Cloning to {llama_cpp_path}...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\", llama_cpp_path], check=True)\n",
    "else:\n",
    "    print(f\"llama.cpp found at {llama_cpp_path}\")\n",
    "\n",
    "# Step 4: Create offload directory\n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "# Step 5: Load base model and tokenizer with disk offloading\n",
    "print(\"Loading base model and tokenizer with disk offloading...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=offload_dir,\n",
    "    offload_state_dict=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step 6: Load LoRA adapters and merge\n",
    "print(\"Loading LoRA adapters and merging...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    "    offload_folder=offload_dir\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Step 7: Save merged model\n",
    "print(f\"Saving merged model to {output_dir}...\")\n",
    "model.save_pretrained(output_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Step 8: Convert merged model to GGUF\n",
    "print(\"Converting merged model to GGUF...\")\n",
    "convert_cmd = [\n",
    "    \"python\",\n",
    "    os.path.join(llama_cpp_path, \"convert_hf_to_gguf.py\"),\n",
    "    output_dir,\n",
    "    \"--outfile\",\n",
    "    gguf_output_path,\n",
    "    \"--outtype\",\n",
    "    \"q8_0\"\n",
    "]\n",
    "result = subprocess.run(convert_cmd, capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"GGUF conversion successful! Output: {gguf_output_path}\")\n",
    "    print(f\"File size: {os.path.getsize(gguf_output_path) / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"GGUF conversion failed!\")\n",
    "    print(\"Error:\", result.stderr)\n",
    "\n",
    "# Step 9: Clean up offload directory\n",
    "print(f\"Cleaning up offload directory {offload_dir}...\")\n",
    "import shutil\n",
    "shutil.rmtree(offload_dir, ignore_errors=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
